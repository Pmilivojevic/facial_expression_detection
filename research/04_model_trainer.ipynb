{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    models: Path\n",
    "    figures: Path\n",
    "    dataset_folder: Path\n",
    "    dataset_labels: Path\n",
    "    model_params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.detmood.constant import *\n",
    "from src.detmood.utils.main_utils import create_directories, read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_file_path = CONFIG_FILE_PATH,\n",
    "        params_file_path = PARAMS_FILE_PATH,\n",
    "        schema_file_path = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        self.schema = read_yaml(schema_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.model\n",
    "        \n",
    "        create_directories([config.models, config.figures])\n",
    "        \n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            models=config.models,\n",
    "            figures=config.figures,\n",
    "            dataset_folder=config.dataset_folder,\n",
    "            dataset_labels=config.dataset_labels,\n",
    "            model_params=params\n",
    "        )\n",
    "        \n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.detmood.constant.dataset_preparation import CustomImageDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def dataset_folds_preparation(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((\n",
    "                self.config.model_params.img_in_size,\n",
    "                self.config.model_params.img_in_size\n",
    "            )),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        dataset = CustomImageDataset(\n",
    "            self.config.dataset_labels,\n",
    "            self.config.dataset_folder,\n",
    "            transform=transform\n",
    "        )\n",
    "        \n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=self.config.model_params.num_folds,\n",
    "            shuffle=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        return dataset, skf\n",
    "    \n",
    "    def validation(\n",
    "            self,\n",
    "            device,\n",
    "            fold,\n",
    "            model,\n",
    "            criterion,\n",
    "            val_loader,\n",
    "            val_losses,\n",
    "            val_accuracies,\n",
    "            epoch,\n",
    "            best_val_loss\n",
    "        ):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print('Validation process...')\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "    \n",
    "            model_path = os.path.join(self.config.models, f'efficientnet_fold_{fold + 1}.pth')\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f'Saved Best Model for Fold {fold + 1} at Epoch {epoch + 1}')\n",
    "            \n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(\n",
    "                cm,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cmap='Blues',\n",
    "                xticklabels=range(self.config.model_params.num_classes),\n",
    "                yticklabels=range(self.config.model_params.num_classes)\n",
    "            )\n",
    "            plt.xlabel('Predicted Labels')\n",
    "            plt.ylabel('True Labels')\n",
    "            plt.title(f'Confusion Matrix for Fold {fold + 1}')\n",
    "            plt.savefig(os.path.join(self.config.figures, f'cm_fold_{fold + 1}.png'))\n",
    "        \n",
    "        return val_losses, val_accuracies, avg_val_loss, val_accuracy\n",
    "    \n",
    "    def train_plot(self, range, train_matric, val_matric, train_label, val_label, fold):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range, train_matric, label=train_label)\n",
    "        plt.plot(range, val_matric, label=val_label)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Train/validation Loss for Fold {fold + 1}')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(self.config.figures, f'Train_Val_{str.split(train_label)[-1]}_Fold_{fold + 1}.png'))\n",
    "    \n",
    "    def train(self):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Device: ', device)\n",
    "        \n",
    "        dataset, skf = self.dataset_folds_preparation()\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in tqdm(enumerate(skf.split(dataset.data_frame, dataset.data_frame['label']))):\n",
    "            print(f'Fold {fold + 1}/{self.config.model_params.num_folds}')\n",
    "            \n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_subset,\n",
    "                batch_size=self.config.model_params.batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subset,\n",
    "                batch_size=self.config.model_params.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "            model.classifier[1] = nn.Sequential(\n",
    "                nn.Linear(\n",
    "                    in_features=1280,\n",
    "                    out_features=512\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(\n",
    "                    in_features=512,\n",
    "                    out_features=self.config.model_params.num_classes\n",
    "                )\n",
    "            )\n",
    "            model.to(device)\n",
    "            \n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.model_params.lr)\n",
    "            \n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            train_accuracies = []\n",
    "            val_accuracies = []\n",
    "            best_val_loss = float('inf')\n",
    "            \n",
    "            for epoch in tqdm(range(self.config.model_params.num_epochs)):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                correct_train = 0\n",
    "                total_train = 0\n",
    "                \n",
    "                for images, labels in tqdm(train_loader):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_train += labels.size(0)\n",
    "                    correct_train += (predicted == labels).sum().item()\n",
    "                    accuracy = 100*((predicted == labels).sum().item()/labels.size(0))\n",
    "                    sys.stdout.write(\"train_loss:%.4f - train_accuracy:%.4f\" %(loss.item(), accuracy))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                avg_train_loss = running_loss / len(train_loader)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                train_accuracy = 100 * correct_train / total_train\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                \n",
    "                val_losses, val_accuracies, avg_val_loss, val_accuracy = self.validation(\n",
    "                    device,\n",
    "                    fold,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    val_loader,\n",
    "                    val_losses,\n",
    "                    val_accuracies,\n",
    "                    epoch,\n",
    "                    best_val_loss\n",
    "                )\n",
    "                \n",
    "                print(f'Epoch [{epoch+1}/{self.config.model_params.num_epochs}], '\n",
    "                      f'Loss: {avg_train_loss:.4f}, '\n",
    "                      f'Validation Loss: {avg_val_loss:.4f}, '\n",
    "                      f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "                      f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "            \n",
    "            epochs_range = range(1, self.config.model_params.num_epochs + 1)\n",
    "            \n",
    "            self.train_plot(\n",
    "                epochs_range,\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                'Train Loss',\n",
    "                'Validation Loss',\n",
    "                fold\n",
    "            )\n",
    "            \n",
    "            self.train_plot(\n",
    "                epochs_range,\n",
    "                train_accuracies,\n",
    "                val_accuracies,\n",
    "                'Train Accuracy',\n",
    "                'Validation Accuracy',\n",
    "                fold\n",
    "            )\n",
    "            \n",
    "            print(f'Finished fold {fold + 1}/{self.config.model_params.num_folds}\\n')\n",
    "        \n",
    "        print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-04 22:19:53,507: INFO: main_utils: created directory at: artifacts]\n",
      "[2024-11-04 22:19:53,509: INFO: main_utils: created directory at: artifacts/model_trainer/models]\n",
      "[2024-11-04 22:19:53,510: INFO: main_utils: created directory at: artifacts/model_trainer/figures]\n",
      "Device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.9566 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.9259 - train_accuracy:12.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.9327 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.9116 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8762 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8226 - train_accuracy:43.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8350 - train_accuracy:28.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7994 - train_accuracy:40.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7812 - train_accuracy:43.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8741 - train_accuracy:28.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7452 - train_accuracy:43.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7914 - train_accuracy:31.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8514 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8494 - train_accuracy:25.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8281 - train_accuracy:34.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7662 - train_accuracy:28.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7691 - train_accuracy:34.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7718 - train_accuracy:28.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7095 - train_accuracy:37.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.6861 - train_accuracy:40.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.6717 - train_accuracy:43.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7292 - train_accuracy:28.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8816 - train_accuracy:15.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8224 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.6587 - train_accuracy:28.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7691 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.7083 - train_accuracy:34.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.5706 - train_accuracy:37.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.6453 - train_accuracy:31.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.6559 - train_accuracy:37.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.8335 - train_accuracy:21.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:1.5703 - train_accuracy:43.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 32/269 [00:12<01:31,  2.58it/s]\n",
      "  0%|          | 0/10 [00:12<?, ?it/s]\n",
      "0it [00:12, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_model_trainer_config()\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_trainer \u001b[39m=\u001b[39m ModelTrainer(config\u001b[39m=\u001b[39mmodel_trainer_config)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32m/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/facial_expression_detection/research/04_model_trainer.ipynb#W5sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m total_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
